import { envs } from "@/config/envs";
import { execSync } from "child_process";

function setupOllama() {
  const modelsList = [
    /*
    "nomic-embed-text",
    "paraphrase-multilingual:latest",
    "embeddinggemma:latest"
    */
   envs.TEXT_EMBEDDING_MODEL_NAME
  ];
  try {
    console.log("üîç Verificando instalaci√≥n de Ollama...");
    
    // Verificar si Ollama est√° instalado
    execSync("ollama --version", { stdio: "pipe" });
    console.log("‚úÖ Ollama est√° instalado");

    modelsList.forEach((modelName) => {
      // Descargar el modelo si no existe
      console.log(`üì• Verificando modelo ${modelName}...`);
      try {
        const models = execSync("ollama list", { encoding: "utf8" });
        if (!models.includes(modelName)) {
          console.log(`‚¨áÔ∏è  Descargando modelo ${modelName}...`);
          execSync(`ollama pull ${modelName}`, { stdio: "inherit" });
          console.log("‚úÖ Modelo descargado correctamente");
        } else {
          console.log("‚úÖ Modelo ya est√° descargado");
        }
      } catch (error) {
        console.log("üöÄ Iniciando servicio Ollama...");
        // En producci√≥n, deber√≠as manejar esto como un servicio
        console.log("üí° Ejecuta en otra terminal: ollama serve");
        console.log("üí° Luego ejecuta: ollama pull nomic-embed-text");
      }
    });
  } catch (error) {
    console.log(`
      ‚ùå Ollama no est√° instalado.
      
      üì• Por favor instala Ollama primero:
      
      macOS: brew install ollama
      Linux: curl -fsSL https://ollama.ai/install.sh | sh
      Windows: Descarga desde https://ollama.ai/download
      
      Luego ejecuta:
      ollama pull nomic-embed-text
    `);
    process.exit(1);
  }
}

setupOllama();
